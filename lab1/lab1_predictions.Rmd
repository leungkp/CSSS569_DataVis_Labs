---
title: "Prediction and visualizing uncertainty: O-ring example"
author: "Brian Leung"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# check if packages are installed; if not, install them
packages <- c("tidyverse", "stargazer", "boot", "MASS", "marginaleffects", "cowplot")
not_installed <- setdiff(packages, rownames(installed.packages()))
if (length(not_installed)) install.packages(not_installed)

# load packages
library(tidyverse)
library(stargazer)
library(boot)
library(marginaleffects)
library(cowplot)
# library(MASS) # don't load MASS package due to conflict w/ dplyr
```

## O-ring data

```{r}
# load data
oring <- read_csv("https://www.openintro.org/data/csv/orings.csv")

# some wrangling 
oring <- 
  oring %>%
  mutate(damaged_dum = if_else(damaged >= 1, 1, 0)) %>%
  rename(temp = temperature)

head(oring)
```

## A brief note on logistic regression

Consider the following logistic regression where we predict the probability of o-ring being damaged using temperature as the predictor:

$$
\text{Pr(Damage|Temp)} = \text{logit}^{-1} (\beta_{0} + \beta_{1} \text{Temp})
$$

More generally, the *link function* that maps the linear predictor $X_{i} \beta$ to the probability $\pi_{i}$ is logit in logistic regression, which is a *non-linear* transformation. We usually prefer to work with the inverse logit. The scale on which we're working is crucial in prediction:

$$
\begin{split}
\text{logit} (\pi_{i}) &= X_{i} \beta \\
\pi_{i} &= \text{logit}^{-1} (X_{i} \beta)
\end{split}
$$ \newpage

## Logit model on O-ring data

```{r}
# logit model 
oring_logit <- glm(damaged_dum ~ temp, data = oring, family = "binomial")

# summary
summary(oring_logit)
```

```{r, results = "asis"}
# regression table w/ stargazer 
stargazer(oring_logit, type = "latex", header = FALSE)
```

\newpage

## Why prediciton and visualization?

Logistical regression, despite its apparent simplicity and ubiquity, is notoriously hard to interpret directly:

-   Logit link function: For every unit increase in $x_{k}$, the log-odds ratio increases by $\beta_{k}$ (???)
    -   Exponentiation helps a bit, but not much...: For every unit increase in $x_{k}$, the odds ratio increases by $e^{\beta_{k}}$
-   Non-linear nature of the link function: for models with multiple predictors, you can't directly interpret a single parameter
    -   The slope on the logistic curve depends on your starting position
-   Probabilities are much more interpretable and substantively meaningful
-   But the problem of incorporating uncertainty into your prediction (e.g. computing confidence intervals)

## Prediction w/ logit model

```{r}
# create hypothetical values for temperature
temp_hypo <- tibble(temp = 20:90) 

# predict prob of damage; be mindful of scale 
damaged_prob <- predict(oring_logit, newdata = temp_hypo, type = "response")

# check the relationship b/w link and response
damaged_link <- predict(oring_logit, newdata = temp_hypo, type = "link")
all.equal(damaged_prob, inv.logit(damaged_link))

# merge prediction w/ hypo values
damaged_pred <- bind_cols(temp_hypo, damaged_prob = damaged_prob)

# visualize w/ ggplot2
ggplot(damaged_pred, aes(x = temp, y = damaged_prob)) +
  geom_line()
```

What is missing from the graph?

\newpage

## Confidence intervals: case of linear regression

```{r}
# use linear regression instead
oring_lm <- lm(damaged_dum ~ temp, data = oring)
damaged_prob_lm <- predict(oring_lm, newdata = temp_hypo, interval = "confidence", level = 0.95)
damaged_pred_lm <- bind_cols(temp_hypo, damaged_prob_lm)

# visualize 
ggplot(damaged_pred_lm, aes(x = temp, y = fit, ymin = lwr, ymax = upr)) +
  geom_line() +
  geom_ribbon(alpha = 0.2)
```

\newpage

## Confidence intervals: case of logit regression (or other GLMs)

```{r}
# predict doesn't work with glm objects in terms of calculating CIs
class(oring_logit)
predict(oring_logit, newdata = temp_hypo, interval = "confidence", level = 0.95)
```

\newpage

## Computing CIs for logit: inverse link function

```{r}
# prediction on logit scale w/ standard errors
link_pred <- predict(oring_logit, newdata = temp_hypo, type = "link", se = TRUE)

# some wrangling
link_pred <- 
  link_pred %>%
  bind_rows() %>%
  select(-residual.scale)

# critical values for 95% and 67% CIs; ignore problem of small-n for simplicity
qnorm(p = (1 - 0.95)/2, lower.tail = FALSE) # ~1.96
qnorm(p = (1 - 0.67)/2, lower.tail = FALSE) # ~0.97

# manually compute CIs: transform linear predictor back to probability via inverse logit 
link_pred_CIs <- 
  link_pred %>%
  mutate(
    pred_prob = inv.logit(fit),
    upr_95 = inv.logit(fit + 1.96 * se.fit),
    lwr_95 = inv.logit(fit - 1.96 * se.fit),
    upr_67 = inv.logit(fit + 0.97 * se.fit),
    lwr_67 = inv.logit(fit - 0.97 * se.fit),
  ) %>%
  bind_cols(temp_hypo)

# visualize w/ ggplot2 
link_pred_vis <- 
 ggplot(link_pred_CIs, aes(x = temp, y = pred_prob, ymin = lwr_67, ymax = upr_67)) +
  geom_line() +
  geom_ribbon(alpha = 0.2) +
  geom_line(aes(y = upr_95), linetype = 2) +
  geom_line(aes(y = lwr_95), linetype = 2)

print(link_pred_vis)
```

\newpage

## Computing CIs for logit: simulation method via MASS::mvrnorm()

Consult Chris's lecture on [Maximum Likelihood](https://faculty.washington.edu/cadolph/mle/topic3.p.pdf) for better reference.

Let's take a step back and think about `predict()` function: how does it calculate the predicted probability?

You can do it by hand. Quick example:

Let's say we want to know the probability of damage given that temperature is 50 degree. We know that the intercept coefficient is 15.0429 and the temperature coefficient is -0.2322.

$$
\begin{split}
\pi_{t=50} &= \text{logit}^{-1} (15.0429 \times 1 + -0.2322 \times 50) \\ 
&\approx 0.9687
\end{split}
$$

We can check the result with `predict()`

```{r}
predict(oring_logit, newdata = data.frame(temp = 50), type = "response")
```

But the problem is that we treat the estimated coefficients as certain and fail to *propagate uncertainty* from our estimation

How can we propagate uncertainty to our prediction? Counterfactual simulation!

Basic logic of counterfactual simulation:

1.  Choose a set of counterfactual value for $x_{c}$
2.  Estimate the model and obtain the parameter vector, $\hat{\beta}$, and its variance covariance matrix, $\hat{V}(\hat{\beta})$
3.  Draw $\tilde{\beta}$ from the multivariate normal $f_{MVN}(\hat{\beta}, \hat{V}(\hat{\beta}))$
4.  Calculate $\tilde{\pi_{c}} = \text{logit}^{-1} (x_{c} \tilde{\beta})$
5.  Repeat the procedure many times, summarizing this vector to get expected values and confidence intervals

```{r}
# point estimate of the parameters
pe <- coef(oring_logit)

# variance covariance of the parameters
vc <- vcov(oring_logit)
 
# set N of simulations
sims <- 10000

# simulate many betas 
sim_beta <- MASS::mvrnorm(sims, pe, vc)

dim(sim_beta)
```

Each row represents one trial in the simulation; there are 10,000 simulations, hence 10,000 rows.

Each column represents one simulated $\tilde{\beta}$; there are two parameters, hence 2 columns.

They encapsulate the uncertainties in our estimation.

Now we can calculate $\tilde{p_{c}}$ with matrix multiplication. To see this:

$$
\underbrace{
  \begin{bmatrix}
  \tilde{\pi_{1, 1}} & \tilde{\pi_{1, 2}} &\dots &\tilde{\pi_{1, n}} \\
  \tilde{\pi_{2, 1}} & \tilde{\pi_{2, 2}} &\dots &\tilde{\pi_{2, n}} \\
  \vdots & \vdots & \ddots & \vdots \\
  \tilde{\pi_{k, 1}} & \tilde{\pi_{2, 2}} &\dots &\tilde{\pi_{k, n}} 
  \end{bmatrix} 
}_\text{k counterfactuals; n simulations}
=
\underbrace{
  \begin{bmatrix}
  1 & x_{c,1} \\
  1 & x_{c,2} \\
  \vdots & \vdots \\ 
  1 & x_{c,k}
  \end{bmatrix}
}_\text{k counterfactural; 2 variables}
\times
\underbrace{
  \begin{bmatrix}
  \tilde{\beta_{0, 1}} & \tilde{\beta_{0, 2}} & \dots & \tilde{\beta_{0, n}} \\
  \tilde{\beta_{1, 1}} & \tilde{\beta_{1, 2}} & \dots & \tilde{\beta_{1, n}} \\
  \end{bmatrix}
}_\text{2 parameters; n simulations}
$$

Intuitively, let's imagine there are $n = 10,000$ parallel universes, each of which is one simulation where $\tilde{\beta}$ exhibits some particular value (from a random MVN draw).

In each parallel universe (simulation), you calculate the particular $\tilde{\pi}$ for each and every counterfactual temparature ${= \{20, 21, 22, \dots, 90\}}$. Essentially, you're repeating the manual calculation we've done above for $k = 71$ times.

Then, you repeat the procedure for each and every parallel universe (simulation).

You should get $n \times k = 10,000 \times 71$ different $\tilde{\pi}$

```{r}
# create hypothetical values for temp; plus constant
hypo_temp <- cbind(1, 20:90)

# check dimensions: 71 
dim(hypo_temp)

# check dimensions for simulated betas
dim(sim_beta)
sim_beta <- t(sim_beta)

# matrix multiplication 
sim_prob <- hypo_temp %*% sim_beta

# check dimensions for simulated probabilities
dim(sim_prob)
```

```{r}
View(sim_prob)
# calculate expected values via mean()
expected_values <- apply(sim_prob, 1, mean)

# calculate confidence intervals via quantile()
CIs_95 <- apply(sim_prob, 1, quantile, prob = c(0.025, 0.975))
CIs_67 <- apply(sim_prob, 1, quantile, prob = c(0.165, 0.835))

# put everything together 
mvrnorm_sim_CIs <- 
  bind_cols(expected_values = expected_values, 
            t(CIs_95), 
            t(CIs_67)) %>%
  mutate_all(inv.logit) %>%
  mutate(hypo_temp = 20:90)

# visualize w/ ggplot2
mvrnorm_sim_vis <- 
  ggplot(mvrnorm_sim_CIs, aes(x = hypo_temp, y = expected_values, ymin = `16.5%`, ymax = `83.5%`)) +
  geom_line() +
  geom_ribbon(alpha = 0.2) +
  geom_line(aes(y = `97.5%`), linetype = 2) +
  geom_line(aes(y = `2.5%`), linetype = 2)

print(mvrnorm_sim_vis)
```

\newpage

## Computing CIs for logit: marginaleffects package

```{r}
# use predictions() function from marginaleffects package
margin_pred_95 <- predictions(oring_logit, newdata = datagrid(temp = 20:90), conf_level = 0.95)
margin_pred_67 <- predictions(oring_logit, newdata = datagrid(temp = 20:90), conf_level = 0.67)

# some wrangling
margin_pred_95 <- 
  margin_pred_95 %>%
  select(temp, predicted, conf.low, conf.high) %>%
  rename(conf_low_95 = conf.low,
         conf_high_95 = conf.high)

margin_pred_67 <- 
  margin_pred_67 %>%
  select(conf.low, conf.high) %>%
  rename(conf_low_67 = conf.low,
         conf_high_67 = conf.high)

# put everything together 
margin_pred_CIs <- cbind(margin_pred_95, margin_pred_67)

# visualize w/ ggplot2
margin_pred_vis <- 
  ggplot(margin_pred_CIs, aes(x = temp, y = predicted, ymin = conf_low_67, ymax = conf_high_67)) +
  geom_line() +
  geom_ribbon(alpha = 0.2) +
  geom_line(aes(y = conf_low_95), linetype = 2) +
  geom_line(aes(y = conf_high_95), linetype = 2)

print(margin_pred_vis)
```

\newpage

## Computing CIs for logit: compare all three methods

```{r}
# use plot_grid() function from cowplot
plot_grid(link_pred_vis, mvrnorm_sim_vis, margin_pred_vis, 
          labels = c("Inverse link function", "Mvrnorm simulation", "marginaleffects package"))
```

\newpage

## Final remarks

-   It's reassuring that all three methods produce equivalent results

-   Hazards of over-relying on off-the-shelf functions or packages: opaque computation can produce unintended, or often wrong, results

    -   Especially when your models become more complex and with more variables

-   Manual simulations can be flexible: e.g. computing first difference and its uncertainties

    -   Given a 10 degree increase in temperature, what is the change in probabilities in damage (and its uncertainties)
    -   Also, a great conceptual check on your fundamental understanding of regression

-   We didn't talk about how to improve the graphs visually

    -   Ugly defaults; no annotation
    -   Also, there are more to the inner working of `ggplot2`
    -   After the lectures have covered more on scientific principles on visual displays, we'll return to this example

## Knitting PDF

You have to install `tinytex` before you can knit a PDF file. Run the following code. We'll talk about LaTeX next week.

```{r}
# install.packages("tinytex")
# tinytex::install_tinytex()
```
